{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNoLHeWky1h4WVMJAbkwYDJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Cs9-AYa1ztqR"},"outputs":[],"source":["import io\n","from google.colab import files"]},{"cell_type":"code","source":[],"metadata":{"id":"Gtrdz_-tGiPQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"iWFjE4Mp0URU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv('/content/fooddata.csv')"],"metadata":{"id":"FOJr8C2k0Y-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the CSV file\n","data = pd.read_csv('/content/fooddata.csv')\n","\n","# Get user input\n","user_input = input(\"Enter a food item: \")\n","\n","# Check if the input exists in the \"Food\" column\n","if user_input in data['Food'].values:\n","    # Get the user item's allergy\n","    user_allergy = data[data['Food'] == user_input]['Allergy'].values[0]\n","\n","    # Find all foods with the same allergy\n","    shared_allergy_foods = data[data['Allergy'] == user_allergy]['Food']\n","\n","    print(\"Food item:\", user_input)\n","    print(\"Allergy:\", user_allergy)\n","    print(\"Other foods with the same allergy:\")\n","    for food in shared_allergy_foods:\n","        if food != user_input:\n","            print(\"- \" + food)\n","else:\n","    print(\"Error: The food item is not found in the CSV file.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gm5BlfVI0ads","executionInfo":{"status":"ok","timestamp":1706625884767,"user_tz":-330,"elapsed":21913,"user":{"displayName":"Nandhiha Vasudevan","userId":"03305051432124428662"}},"outputId":"9346d26f-632a-401e-fd24-a59542a4b591"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter a food item: Honey\n","Food item: Honey\n","Allergy: Honey Allergy\n","Other foods with the same allergy:\n","- Royal Jelly\n"]}]},{"cell_type":"code","source":["data = pd.read_csv('/content/finalrecipe.csv')"],"metadata":{"id":"jYgibkl81LTY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the CSV file\n","data = pd.read_csv('/content/fooddata.csv')\n","\n","# Get user input\n","user_input = input(\"Enter a food item: \")\n","\n","# Check if the input exists in the \"Food\" column\n","if user_input in data['Food'].values:\n","    # Get the user item's allergy\n","    user_allergy = data[data['Food'] == user_input]['Allergy'].values[0]\n","\n","    # Find all foods with the same allergy\n","    shared_allergy_foods = data[data['Allergy'] == user_allergy]['Food']\n","\n","    print(\"Food item:\", user_input)\n","    print(\"Allergy:\", user_allergy)\n","    print(\"Other foods with the same allergy:\")\n","    for food in shared_allergy_foods:\n","        if food != user_input:\n","            print(\"- \" + food)\n","\n","    # Ask the user to upload the final recipe CSV file\n","    uploaded = False\n","    while not uploaded:\n","        try:\n","            final_recipe_data = pd.read_csv('/content/finalrecipe.csv')\n","            uploaded = True\n","        except FileNotFoundError:\n","            print(\"Error: 'finalrecipe.csv' not found. Please upload the file.\")\n","            uploaded = True\n","\n","    # Check if foods with the same allergy exist in the final recipe's Main Ingredient column\n","    matching_foods_final_recipe = final_recipe_data[final_recipe_data['Main Ingredient'].isin(shared_allergy_foods)]\n","\n","    if not matching_foods_final_recipe.empty:\n","        print(\"\\nFoods with the same allergy found in 'finalrecipe.csv' Main Ingredient:\")\n","        for index, row in matching_foods_final_recipe.iterrows():\n","            print(\"- \" + row['Main Ingredient'] + \": \" + row['Food Product'])\n","    else:\n","        print(\"\\nNo matching foods found in 'finalrecipe.csv' Main Ingredient.\")\n","else:\n","    print(\"Error: The food item is not found in the CSV file.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lyq-VAP36jOS","executionInfo":{"status":"ok","timestamp":1706627838938,"user_tz":-330,"elapsed":4915,"user":{"displayName":"Nandhiha Vasudevan","userId":"03305051432124428662"}},"outputId":"1e485cce-a2db-409b-95b4-e42380075320"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter a food item: Apple\n","Food item: Apple\n","Allergy: Oral Allergy Syndrome\n","Other foods with the same allergy:\n","- Avocado\n","- Date\n","- Guava\n","- Japanese pear\n","- Kiwi\n","- Loquat\n","- Mango\n","- Papaya\n","- Passion fruit\n","- Pear\n","- Pineapple\n","- Quince\n","\n","Foods with the same allergy found in 'finalrecipe.csv' Main Ingredient:\n","- Mango: Mango Lassi\n","- Avocado: Avocado Toast\n","- Mango: Mango Salsa\n","- Mango: Mango Coconut Popsicles\n","- Mango: Mango Salsa\n","- Avocado: Avocado Toast\n","- Mango: Mango Coconut Popsicles\n","- Mango: Mango Salsa\n","- Pineapple: Pineapple Upside-Down Cake\n"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","# Generate some example data\n","np.random.seed(42)\n","X = 2 * np.random.rand(100, 1)\n","y = 4 + 3 * X + np.random.randn(100, 1)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Linear Regression model\n","model = LinearRegression()\n","\n","# Train the model on the training set\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","predictions = model.predict(X_test)\n","\n","# Evaluate the model\n","mse = mean_squared_error(y_test, predictions)\n","print(f'Mean Squared Error: {mse}')\n","\n","# Now the 'model' variable contains the trained linear regression model\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iP0_FORzGkIf","executionInfo":{"status":"ok","timestamp":1706680818967,"user_tz":-330,"elapsed":2495,"user":{"displayName":"Nandhiha Vasudevan","userId":"03305051432124428662"}},"outputId":"af82a59f-37af-43b8-e99a-fd4d911881f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error: 0.6536995137170021\n"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.metrics import r2_score\n","\n","# ...\n","\n","# Evaluate the model using R-squared (R2) score\n","r2 = r2_score(y_test, predictions)\n","print(f'R-squared (R2) Score: {r2}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TncXNhZ0GsBb","executionInfo":{"status":"ok","timestamp":1706680861116,"user_tz":-330,"elapsed":864,"user":{"displayName":"Nandhiha Vasudevan","userId":"03305051432124428662"}},"outputId":"eee71808-60aa-44fe-9fe4-ae43b6e3ea9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["R-squared (R2) Score: 0.8072059636181392\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# Generate synthetic classification data\n","np.random.seed(42)\n","X = 2 * np.random.rand(100, 2)\n","y = (X[:, 0] + X[:, 1] > 1).astype(int)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model\n","model = LogisticRegression()\n","\n","# Train the model on the training set\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","predictions = model.predict(X_test)\n","\n","# Evaluate the model using accuracy\n","accuracy = accuracy_score(y_test, predictions)\n","print(f'Accuracy: {accuracy}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9O61MVWG09U","executionInfo":{"status":"ok","timestamp":1706680886278,"user_tz":-330,"elapsed":634,"user":{"displayName":"Nandhiha Vasudevan","userId":"03305051432124428662"}},"outputId":"fe23b370-d496-4e43-9272-b6b92cacdbea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.85\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neural_network import MLPClassifier\n","from xgboost import XGBClassifier\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.gaussian_process.kernels import RBF\n","import numpy as np\n","\n","# Generate synthetic classification data\n","np.random.seed(42)\n","X = 2 * np.random.rand(100, 2)\n","y = (X[:, 0] + X[:, 1] > 1).astype(int)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Naive Bayes\n","naive_bayes_model = GaussianNB()\n","naive_bayes_model.fit(X_train, y_train)\n","naive_bayes_predictions = naive_bayes_model.predict(X_test)\n","naive_bayes_accuracy = accuracy_score(y_test, naive_bayes_predictions)\n","print(f'Naive Bayes Accuracy: {naive_bayes_accuracy}')\n","\n","# Decision Tree\n","decision_tree_model = DecisionTreeClassifier()\n","decision_tree_model.fit(X_train, y_train)\n","decision_tree_predictions = decision_tree_model.predict(X_test)\n","decision_tree_accuracy = accuracy_score(y_test, decision_tree_predictions)\n","print(f'Decision Tree Accuracy: {decision_tree_accuracy}')\n","\n","# Random Forest\n","random_forest_model = RandomForestClassifier()\n","random_forest_model.fit(X_train, y_train)\n","random_forest_predictions = random_forest_model.predict(X_test)\n","random_forest_accuracy = accuracy_score(y_test, random_forest_predictions)\n","print(f'Random Forest Accuracy: {random_forest_accuracy}')\n","\n","# Support Vector Machines (SVM)\n","svm_model = SVC()\n","svm_model.fit(X_train, y_train)\n","svm_predictions = svm_model.predict(X_test)\n","svm_accuracy = accuracy_score(y_test, svm_predictions)\n","print(f'SVM Accuracy: {svm_accuracy}')\n","\n","# K-Nearest Neighbors (KNN)\n","knn_model = KNeighborsClassifier()\n","knn_model.fit(X_train, y_train)\n","knn_predictions = knn_model.predict(X_test)\n","knn_accuracy = accuracy_score(y_test, knn_predictions)\n","print(f'KNN Accuracy: {knn_accuracy}')\n","\n","# Neural Networks\n","nn_model = MLPClassifier()\n","nn_model.fit(X_train, y_train)\n","nn_predictions = nn_model.predict(X_test)\n","nn_accuracy = accuracy_score(y_test, nn_predictions)\n","print(f'Neural Network Accuracy: {nn_accuracy}')\n","\n","# Gradient Boosting Models (XGBoost)\n","xgb_model = XGBClassifier()\n","xgb_model.fit(X_train, y_train)\n","xgb_predictions = xgb_model.predict(X_test)\n","xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n","print(f'XGBoost Accuracy: {xgb_accuracy}')\n","\n","# Ensemble Methods (Voting Classifier)\n","ensemble_model = VotingClassifier(estimators=[('svm', svm_model), ('knn', knn_model), ('nn', nn_model), ('xgb', xgb_model)])\n","ensemble_model.fit(X_train, y_train)\n","ensemble_predictions = ensemble_model.predict(X_test)\n","ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n","print(f'Ensemble Methods Accuracy: {ensemble_accuracy}')\n","\n","# Gaussian Processes\n","gp_model = GaussianProcessClassifier(kernel=RBF())\n","gp_model.fit(X_train, y_train)\n","gp_predictions = gp_model.predict(X_test)\n","gp_accuracy = accuracy_score(y_test, gp_predictions)\n","print(f'Gaussian Processes Accuracy: {gp_accuracy}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ji2xvdBI-9A","executionInfo":{"status":"ok","timestamp":1706681436118,"user_tz":-330,"elapsed":2462,"user":{"displayName":"Nandhiha Vasudevan","userId":"03305051432124428662"}},"outputId":"b354391d-b0e9-40b8-f47b-14d08a9cae8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Bayes Accuracy: 0.95\n","Decision Tree Accuracy: 0.95\n","Random Forest Accuracy: 0.95\n","SVM Accuracy: 0.95\n","KNN Accuracy: 0.95\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Neural Network Accuracy: 0.85\n","XGBoost Accuracy: 0.85\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Ensemble Methods Accuracy: 0.95\n","Gaussian Processes Accuracy: 0.95\n"]}]},{"cell_type":"code","source":["!pip install gradio\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rua7oQh2JUpC","executionInfo":{"status":"ok","timestamp":1706681566965,"user_tz":-330,"elapsed":36840,"user":{"displayName":"Nandhiha Vasudevan","userId":"03305051432124428662"}},"outputId":"a63f5353-2236-4932-e5d9-98bf5abc0965"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-4.16.0-py3-none-any.whl (16.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n","Collecting fastapi (from gradio)\n","  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpy (from gradio)\n","  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.8.1 (from gradio)\n","  Downloading gradio_client-0.8.1-py3-none-any.whl (305 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx (from gradio)\n","  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.9.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n","Collecting pydantic>=2.0 (from gradio)\n","  Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting python-multipart (from gradio)\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n","Collecting ruff>=0.1.7 (from gradio)\n","  Downloading ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.8.1->gradio) (2023.6.0)\n","Collecting websockets<12.0,>=10.0 (from gradio-client==0.8.1->gradio)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.47.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n","Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio)\n","  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n","Collecting pydantic-core==2.16.1 (from pydantic>=2.0->gradio)\n","  Downloading pydantic_core-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions~=4.0 (from gradio)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n","Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n","Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting starlette<0.36.0,>=0.35.0 (from fastapi->gradio)\n","  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2023.11.17)\n","Collecting httpcore==1.* (from httpx->gradio)\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.6)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.17.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio) (1.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n","Building wheels for collected packages: ffmpy\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=dccd53adc10e41322f35d64a5aa3e4c7a898dfe5e0b83dd1beddaeb27879f26b\n","  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n","Successfully built ffmpy\n","Installing collected packages: pydub, ffmpy, websockets, typing-extensions, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, annotated-types, aiofiles, uvicorn, starlette, pydantic-core, httpcore, pydantic, httpx, gradio-client, fastapi, gradio\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 1.10.14\n","    Uninstalling pydantic-1.10.14:\n","      Successfully uninstalled pydantic-1.10.14\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiofiles-23.2.1 annotated-types-0.6.0 colorama-0.4.6 fastapi-0.109.0 ffmpy-0.3.1 gradio-4.16.0 gradio-client-0.8.1 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 orjson-3.9.12 pydantic-2.6.0 pydantic-core-2.16.1 pydub-0.25.1 python-multipart-0.0.6 ruff-0.1.15 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.35.1 tomlkit-0.12.0 typing-extensions-4.9.0 uvicorn-0.27.0.post1 websockets-11.0.3\n"]}]},{"cell_type":"code","source":["!pip install typing_extensions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Rop2GLLJ8R5","executionInfo":{"status":"ok","timestamp":1706681695803,"user_tz":-330,"elapsed":9067,"user":{"displayName":"Nandhiha Vasudevan","userId":"03305051432124428662"}},"outputId":"2138c0e0-27ee-4a0d-da5e-eb254b84a63c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (4.9.0)\n"]}]},{"cell_type":"code","source":["import os\n","os.kill(os.getpid(), 9)"],"metadata":{"id":"nPNYZF6gKBQv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","import pandas as pd\n","\n","# Load the CSV file\n","data = pd.read_csv('/content/fooddata.csv')\n","\n","def find_matching_foods(user_input):\n","    if user_input in data['Food'].values:\n","        # Get the user item's allergy\n","        user_allergy = data[data['Food'] == user_input]['Allergy'].values[0]\n","\n","        # Find all foods with the same allergy\n","        shared_allergy_foods = data[data['Allergy'] == user_allergy]['Food']\n","\n","        result = [\n","            f\"FOOD ITEM: {user_input}\\n\",\n","            f\"ALLERGY: {user_allergy}\\n\",\n","            f\"OTHER FOOD WITH SAME ALLERGY: {list(shared_allergy_foods)}\\n\"\n","        ]\n","\n","        # Ask the user to upload the final recipe CSV file\n","        try:\n","            final_recipe_data = pd.read_csv('/content/finalrecipe.csv')\n","            # Check if foods with the same allergy exist in the final recipe's Main Ingredient column\n","            matching_foods_final_recipe = final_recipe_data[final_recipe_data['Main Ingredient'].isin(shared_allergy_foods)]\n","\n","            if not matching_foods_final_recipe.empty:\n","                result.append(f\"Matching foods:  {list(matching_foods_final_recipe['Food Product'])}\\n\")\n","            else:\n","                result.append(\"No matching foods found in 'finalrecipe.csv' Main Ingredient: None\")\n","\n","        except FileNotFoundError:\n","            result.append(\"'finalrecipe.csv' not found. Please upload the file.\")\n","\n","        return '\\n'.join(result)\n","    else:\n","        return \"Error: The food item is not found in the CSV file.\"\n","\n","iface = gr.Interface(\n","    fn=find_matching_foods,\n","    inputs=gr.Textbox(label=\"Enter a food item:\"),\n","    outputs=\"text\"\n",")\n","\n","iface.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"id":"3nTT0L8uOwy8","executionInfo":{"status":"ok","timestamp":1706699732460,"user_tz":-330,"elapsed":8776,"user":{"displayName":"Nandhiha Vasudevan","userId":"03305051432124428662"}},"outputId":"ca85ed2c-b72e-4a3a-a8ce-b32235e22d76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://23f56cc34999d83131.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://23f56cc34999d83131.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["!pip install scikit-learn-intelex"],"metadata":{"id":"Me8i6QMChczt"},"execution_count":null,"outputs":[]}]}